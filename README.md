# API Predict - Emarsys Scraper

Orquestador de scraping para productos recomendados de Extend Emarsys usando Playwright, Node.js y NestJS.

## Caracter√≠sticas

- **Scraping robusto**: Utiliza Playwright para scraping confiable de contenido din√°mico
- **Recomendaciones por CPF**: Integraci√≥n con Emarsys Scarab para obtener productos recomendados basados en CPF
- **Arquitectura as√≠ncrona**: Sistema de colas con Bull para manejar m√∫ltiples tareas de scraping
- **Cach√© inteligente**: Sistema de cach√© para evitar scraping repetitivo
- **API REST completa**: Endpoints bien documentados con Swagger
- **TypeScript**: Tipado fuerte para mejor mantenibilidad
- **Reintentos autom√°ticos**: Manejo de errores con reintentos exponenciales
- **Monitoreo**: Sistema de logging y estad√≠sticas de la cola
- **M√∫ltiples l√≥gicas**: Soporte para PERSONAL, RELATED, ALSO_BOUGHT, POPULAR, CART, etc.

## Requisitos previos

- Node.js >= 18
- Redis (para las colas de Bull)
- npm o yarn

## Instalaci√≥n

1. Instalar dependencias:
```bash
npm install
```

2. Instalar Playwright browsers:
```bash
npx playwright install chromium
```

3. Configurar variables de entorno:
```bash
cp .env.example .env
```

Editar el archivo `.env` con tus credenciales:
```env
PORT=3000
EMARSYS_URL=https://extend.emarsys.com
EMARSYS_USERNAME=tu_usuario
EMARSYS_PASSWORD=tu_contrase√±a
EMARSYS_SCARAB_ID=tu_scarab_id
REDIS_HOST=localhost
REDIS_PORT=6379
API_KEYS=your-secret-api-key,another-key
```

4. Asegurarse de que Redis est√© corriendo:
```bash
# En macOS con Homebrew
brew services start redis

# En Linux
sudo service redis-server start

# Con Docker
docker run -d -p 6379:6379 redis
```

## Uso

### Desarrollo
```bash
npm run start:dev
```

### Producci√≥n
```bash
npm run build
npm run start:prod
```

### Testing
```bash
npm run test
```

## üîê Autenticaci√≥n

La API est√° protegida mediante **API Keys**. Para acceder a los endpoints protegidos, necesitas incluir tu API key en cada request.

### M√©todos de autenticaci√≥n soportados:

#### 1. Header X-API-Key (Recomendado)
```bash
curl -X POST http://localhost:3000/api/scraping/recommendations/cpf \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your-secret-api-key" \
  -d '{"cpf": "12345678900", "recommendLogic": "PERSONAL"}'
```

#### 2. Bearer Token
```bash
curl -X POST http://localhost:3000/api/scraping/recommendations/cpf \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-secret-api-key" \
  -d '{"cpf": "12345678900", "recommendLogic": "PERSONAL"}'
```

#### 3. Query Parameter
```bash
curl -X POST "http://localhost:3000/api/scraping/recommendations/cpf?api_key=your-secret-api-key" \
  -H "Content-Type: application/json" \
  -d '{"cpf": "12345678900", "recommendLogic": "PERSONAL"}'
```

### Configurar API Keys

Edita el archivo `.env` y agrega tus API keys separadas por comas:
```env
API_KEYS=key1,key2,key3
```

### Endpoints p√∫blicos

El siguiente endpoint NO requiere autenticaci√≥n:
- `GET /api/scraping/stats` - Estad√≠sticas de la cola (p√∫blico)

### Endpoints protegidos

Los siguientes endpoints requieren autenticaci√≥n:
- `POST /api/scraping/start` - Iniciar scraping
- `POST /api/scraping/recommendations/cpf` - Obtener recomendaciones por CPF
- `GET /api/scraping/jobs/:id` - Obtener estado de un job
- `GET /api/scraping/jobs` - Listar todos los jobs
- `DELETE /api/scraping/jobs/:id` - Cancelar un job
- `POST /api/scraping/jobs/:id/retry` - Reintentar un job
- `DELETE /api/scraping/jobs/completed/clear` - Limpiar jobs completados

## üìö Documentaci√≥n de la API

Hay **dos formas** de explorar la documentaci√≥n:

### 1. Swagger UI (Interactivo - para probar la API)
```
http://localhost:3000/api/docs
```
‚úÖ Probar endpoints directamente
‚úÖ Ejecutar requests
‚úÖ Ver respuestas en tiempo real

### 2. Redocly (Documentaci√≥n Bonita - solo CPF endpoint)
```bash
# Opci√≥n A: Servidor live
npm run docs:serve
# Abrir: http://localhost:8080

# Opci√≥n B: HTML standalone
open docs/index.html
```
‚úÖ Interface moderna e intuitiva
‚úÖ Ejemplos detallados
‚úÖ Busca integrada
‚úÖ Mobile-friendly

üëâ **Ver gu√≠a completa**: [REDOCLY_GUIDE.md](REDOCLY_GUIDE.md)

### Principales endpoints:

#### üî• Obtener recomendaciones por CPF (NUEVO)
```http
POST /api/scraping/recommendations/cpf
Content-Type: application/json
X-API-Key: your-secret-api-key

{
  "cpf": "123.456.789-00",
  "scarabId": "1916F613C8B45191",
  "recommendLogic": "PERSONAL",
  "limit": 10,
  "headless": true,
  "useCache": true
}
```

Respuesta:
```json
{
  "id": "1",
  "status": "pending",
  "startedAt": "2024-01-20T10:00:00.000Z"
}
```

**L√≥gicas disponibles**: `PERSONAL`, `RELATED`, `ALSO_BOUGHT`, `POPULAR`, `CATEGORY`, `CART`

Ver la [Gu√≠a Completa de CPF](CPF_GUIDE.md) para m√°s detalles.

#### Iniciar scraping
```http
POST /api/scraping/start
Content-Type: application/json
X-API-Key: your-secret-api-key

{
  "url": "https://extend.emarsys.com/products/recommended",
  "username": "usuario@example.com",
  "password": "password",
  "maxProducts": 50,
  "timeout": 30000,
  "headless": true,
  "useCache": true
}
```

Respuesta:
```json
{
  "id": "1",
  "status": "pending",
  "startedAt": "2024-01-20T10:00:00.000Z"
}
```

#### Obtener estado de un job
```http
GET /api/scraping/jobs/{id}
X-API-Key: your-secret-api-key
```

Respuesta:
```json
{
  "id": "1",
  "status": "completed",
  "startedAt": "2024-01-20T10:00:00.000Z",
  "completedAt": "2024-01-20T10:00:30.000Z",
  "result": {
    "success": true,
    "products": [
      {
        "id": "prod-123",
        "name": "Producto Ejemplo",
        "description": "Descripci√≥n del producto",
        "price": 99.99,
        "currency": "USD",
        "imageUrl": "https://...",
        "url": "https://...",
        "scrapedAt": "2024-01-20T10:00:30.000Z"
      }
    ],
    "totalProducts": 1,
    "scrapedAt": "2024-01-20T10:00:30.000Z",
    "duration": 30000
  }
}
```

#### Listar todos los jobs
```http
GET /api/scraping/jobs
```

#### Cancelar un job
```http
DELETE /api/scraping/jobs/{id}
```

#### Reintentar un job fallido
```http
POST /api/scraping/jobs/{id}/retry
```

#### Estad√≠sticas de la cola
```http
GET /api/scraping/stats
```

## Arquitectura

```
src/
‚îú‚îÄ‚îÄ main.ts                 # Punto de entrada
‚îú‚îÄ‚îÄ app.module.ts          # M√≥dulo ra√≠z
‚îú‚îÄ‚îÄ common/                # C√≥digo compartido
‚îÇ   ‚îú‚îÄ‚îÄ dto/              # Data Transfer Objects
‚îÇ   ‚îú‚îÄ‚îÄ filters/          # Filtros de excepciones
‚îÇ   ‚îú‚îÄ‚îÄ interceptors/     # Interceptores
‚îÇ   ‚îî‚îÄ‚îÄ interfaces/       # Interfaces TypeScript
‚îú‚îÄ‚îÄ scraper/              # M√≥dulo de scraping
‚îÇ   ‚îú‚îÄ‚îÄ scraper.service.ts    # L√≥gica de scraping con Playwright
‚îÇ   ‚îî‚îÄ‚îÄ scraper.module.ts
‚îî‚îÄ‚îÄ orchestrator/         # M√≥dulo de orquestaci√≥n
    ‚îú‚îÄ‚îÄ orchestrator.service.ts      # L√≥gica de orquestaci√≥n
    ‚îú‚îÄ‚îÄ orchestrator.controller.ts   # Endpoints REST
    ‚îú‚îÄ‚îÄ orchestrator.processor.ts    # Procesador de cola Bull
    ‚îî‚îÄ‚îÄ orchestrator.module.ts
```

## Configuraci√≥n avanzada

### Personalizar selectores de scraping

El scraper usa selectores autom√°ticos, pero puedes personalizarlos editando [src/scraper/scraper.service.ts](src/scraper/scraper.service.ts):

```typescript
const productSelectors = [
  '.product-card',
  '.product-item',
  // Agregar tus selectores personalizados
];
```

### Ajustar reintentos y timeouts

En [.env](.env):
```env
SCRAPING_TIMEOUT=60000      # Timeout por job
RETRY_ATTEMPTS=3            # N√∫mero de reintentos
RETRY_DELAY=5000           # Delay entre reintentos
MAX_CONCURRENT_SCRAPERS=3   # Jobs concurrentes
```

### Configurar cach√©

```env
CACHE_TTL=3600             # Tiempo de vida en segundos
CACHE_MAX_ITEMS=100        # M√°ximo items en cach√©
```

## Manejo de errores

El sistema incluye:
- Reintentos autom√°ticos con backoff exponencial
- Logging detallado de errores
- Filtros de excepciones globales
- Validaci√≥n de entrada con class-validator

## Monitoreo

### Ver logs en desarrollo:
Los logs se muestran en la consola con informaci√≥n sobre:
- Inicio/fin de jobs
- Errores de scraping
- Estad√≠sticas de rendimiento

### M√©tricas de la cola:
```http
GET /api/scraping/stats
```

Retorna:
```json
{
  "waiting": 5,
  "active": 2,
  "completed": 100,
  "failed": 3,
  "delayed": 0,
  "total": 110
}
```

## Mejores pr√°cticas

1. **Usar cach√©**: Habilitado por defecto para evitar scraping repetitivo
2. **Modo headless**: Usar `headless: true` en producci√≥n para mejor rendimiento
3. **Limitar productos**: Usar `maxProducts` para evitar timeouts
4. **Monitorear la cola**: Revisar regularmente las estad√≠sticas
5. **Limpiar jobs**: Usar el endpoint de limpieza peri√≥dicamente

## Troubleshooting

### Error: Redis connection refused
```bash
# Verificar que Redis est√© corriendo
redis-cli ping
# Deber√≠a responder: PONG
```

### Error: Playwright browser not found
```bash
npx playwright install chromium
```

### Jobs atascados en "pending"
```bash
# Verificar workers de Bull
# Revisar logs para errores
# Reiniciar Redis si es necesario
```

## Seguridad

- ‚úÖ **Autenticaci√≥n habilitada**: Todos los endpoints (excepto /stats) requieren API Key
- No commitear el archivo `.env` con credenciales reales
- Usar API keys fuertes y √∫nicas en producci√≥n
- Rotar API keys peri√≥dicamente
- Usar variables de entorno en producci√≥n
- Validar y sanitizar todas las entradas
- El endpoint `/api/scraping/stats` es p√∫blico para monitoreo

## Licencia

MIT

## Soporte

Para reportar issues o solicitar features, crear un issue en el repositorio.
